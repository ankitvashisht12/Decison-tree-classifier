{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Algorithm\n",
    "- Implementation of Decision Tree Algorithm on Iris DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math as m\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 4)\n",
      "(38, 4)\n",
      "(112,)\n",
      "(112,)\n"
     ]
    }
   ],
   "source": [
    "Iris = datasets.load_iris()\n",
    "X = Iris.data\n",
    "Y = Iris.target\n",
    "\n",
    "X_train, X_test, Y_train , Y_test  = train_test_split(X, Y)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, level = 0):\n",
    "        self.level = 0\n",
    "        self.featureName = ['sepal length (cm)','sepal width (cm)','petal length (cm)','petal width (cm)'] \n",
    "    \n",
    "    #####  transform_data : Merging Xtrain and Ytrain to form data\n",
    "    def transform_data(self, X_train , Y_train):\n",
    "        Y_train = Y_train.reshape(len(Y_train),1)\n",
    "        data = np.concatenate((X_train,Y_train), axis=1)\n",
    "        return data\n",
    "    \n",
    "    ##### Check Purity  - To check if node is leaf node or not\n",
    "    def check_purity(self, data):\n",
    "        label_column = [int(i) for i in data[:,-1]]\n",
    "        unique_classes = np.unique(label_column)\n",
    "        if(len(unique_classes) == 1):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    ##### Classify data - It classifies which class does node belong if node is leaf node    \n",
    "    def classify_data(self, data):\n",
    "        label_column = [int(i) for i in data[:,-1]]\n",
    "        unique_classes, count_unique_classes = np.unique(label_column, return_counts=True)\n",
    "        classification = unique_classes[count_unique_classes.argmax()]\n",
    "        return classification\n",
    "   \n",
    "    ##### Get Potential Splits - It returns dictionary that contains columns as keys and average potential sorted values as values of dictionary\n",
    "    def get_potential_split(self, data):\n",
    "        potential_splits = {}\n",
    "        n_columns = len(data[0])-1\n",
    "        for c in range(n_columns):\n",
    "            potential_splits[c] = []\n",
    "            values = data[:, c]\n",
    "            unique_values = np.unique(values)\n",
    "\n",
    "            for i in range(1,len(unique_values)):\n",
    "                ps = (unique_values[i] + unique_values[i-1]) / 2\n",
    "                potential_splits[c].append(ps)\n",
    "        return potential_splits\n",
    "    \n",
    "    # split_data : to split data into two parts i.e. data_left and data_right by some criteria i.e. split_value and split_column\n",
    "    def split_data(self, data, split_column, split_value):\n",
    "        split_column_values = data[:, split_column]\n",
    "\n",
    "        data_left = data[split_column_values <= split_value]\n",
    "        data_right = data[split_column_values > split_value]\n",
    "\n",
    "        return data_left, data_right\n",
    "    \n",
    "    ##### Entropy, Infomation Required , Split Information\n",
    "          # - These helper function calculate and return entropy , information required , split information which is used to calculate gain ratio of particular node\n",
    "          # - Entropy = -Summation(p(i)*log2(p(i)))\n",
    "    def entropy(self, data):\n",
    "        labels_values = [int(i) for i in data[:,-1]]\n",
    "        _, counts = np.unique(labels_values, return_counts=True)\n",
    "        probabilities = counts / counts.sum()\n",
    "        entropies = sum(probabilities * -np.log2(probabilities))\n",
    "        return entropies\n",
    "\n",
    "    def info_req(self, data_left, data_right):\n",
    "        n_total = len(data_left) + len(data_right)\n",
    "        p_left = len(data_left) / n_total\n",
    "        p_right = len(data_right) / n_total\n",
    "        return (p_left * self.entropy(data_left) + p_right* self.entropy(data_right))\n",
    "\n",
    "    def split_info(self, data_left, data_right):\n",
    "        n_total= len(data_left) + len(data_right)\n",
    "        return (- (len(data_left) / n_total) * np.log(len(data_left) / n_total) - (len(data_right) / n_total) * np.log(len(data_right) / n_total))\n",
    "    \n",
    "    ##### Gain Ratio - Returns gain ratio for particular node\n",
    "    def gain_ratio(self, data, data_left, data_right):\n",
    "        si = self.split_info(data_left, data_right)\n",
    "        ig = self.entropy(data) - self.info_req(data_left, data_right)\n",
    "        return (ig / si)\n",
    "    \n",
    "    ##### Find Best Split Helper Function - This function takes Xtrain data and potential splits dictionary and \n",
    "    #                                                        returns the best split column and best split value\n",
    "    def find_best_split(self, data, potential_splits):\n",
    "        max_entropy = m.inf\n",
    "        max_gain = -m.inf\n",
    "        for c in potential_splits:\n",
    "            for v in potential_splits[c]:\n",
    "                data_left, data_right = self.split_data(data, split_column=c, split_value=v)\n",
    "                current_entropy = self.info_req(data_left, data_right)\n",
    "                current_gain = self.gain_ratio(data, data_left , data_right)\n",
    "\n",
    "\n",
    "                if current_entropy < max_entropy:\n",
    "                    max_gain = current_gain\n",
    "                    max_entropy = current_entropy\n",
    "                    best_split_column = c\n",
    "                    best_split_value = v\n",
    "        return best_split_column, best_split_value \n",
    "\n",
    "    '''\n",
    "    data = Xtrain + Ytrain\n",
    "    f = features\n",
    "    level = current level\n",
    "    \n",
    "    decision_tree function is being called recursively to return binary sub_tree\n",
    "    '''\n",
    "    def decision_tree(self, data, f, level):\n",
    "\n",
    "        if self.check_purity(data) :\n",
    "            labels_values = [int(i) for i in data[:,-1]]\n",
    "            _, counts = np.unique(labels_values, return_counts=True)\n",
    "            print('Level', level)\n",
    "            for i in range(len(counts)):\n",
    "                print('Count of', i, '= ', counts[i])\n",
    "                print('Current Entropy  is = ', self.entropy(data))\n",
    "                print('Reached leaf Node')\n",
    "            return self.classify_data(data)\n",
    "\n",
    "\n",
    "        elif len(f) == 0:\n",
    "            labels_values = [int(i) for i in data[:,-1]]\n",
    "            _, counts = np.unique(labels_values, return_counts=True)\n",
    "            print('Level', level)\n",
    "            for i in range(len(counts)):\n",
    "                print('Count of', i, '= ', counts[i])\n",
    "                print('Current Entropy  is = ', self.entropy(data))\n",
    "                print('Reached leaf Node')\n",
    "            return self.classify_data(data)\n",
    "\n",
    "\n",
    "        else:\n",
    "            potential_split = self.get_potential_split(data)\n",
    "            best_split_feature, best_split_value = self.find_best_split(data, potential_split)\n",
    "            data_left, data_right = self.split_data(data, best_split_feature, best_split_value)\n",
    "\n",
    "            labels_values = [int(i) for i in data[:,-1]]\n",
    "            _, counts = np.unique(labels_values, return_counts=True)\n",
    "            print('Level', level)\n",
    "            for i in range(len(counts)):\n",
    "                print('Count of', i, '= ', counts[i])\n",
    "                print('Current Entropy  is = ', self.entropy(data))\n",
    "                print('Splitting on feature', f[best_split_feature], 'with gain ratio', self.gain_ratio(data, data_left, data_right))\n",
    "\n",
    "            data_left = np.delete(data_left, best_split_feature , 1)\n",
    "            data_right = np.delete(data_right, best_split_feature , 1)\n",
    "\n",
    "            que = '{} <= {}'.format(f[best_split_feature], best_split_value)\n",
    "            sub_tree = {que : []}\n",
    "\n",
    "            f.remove(f[best_split_feature])\n",
    "            y = self.decision_tree(data_left, f, level+1)\n",
    "            n = self.decision_tree(data_right, f, level+1)\n",
    "\n",
    "            sub_tree[que].append(y)\n",
    "            sub_tree[que].append(n)\n",
    "            return sub_tree\n",
    "\n",
    "    ### Fit Function - fits training dataset \n",
    "    #                - Returns dictionary generated from decision tree\n",
    "    def fit(self, Xtrain , Ytrain):\n",
    "        self.data = self.transform_data(Xtrain , Ytrain)\n",
    "        self.dictionary = self.decision_tree(self.data, self.featureName, self.level)               # here data is X_Y_train\n",
    "    \n",
    "    '''\n",
    "    predict_row() is helper function which takes one row of testing dataset and dictionary.\n",
    "    It returns class for particular testing point / row.\n",
    "    '''\n",
    "\n",
    "    def predict_row(self, x, dictionary):\n",
    "    \n",
    "        col = {'petal width (cm)':3, 'sepal length (cm)':0 , 'sepal width (cm)':1 , 'petal length (cm)' :2}\n",
    "\n",
    "        q = list(dictionary.keys())[0]\n",
    "        feature1,feature2,feature3, operator, value = q.split()\n",
    "        feature = feature1+\" \"+feature2+\" \"+feature3\n",
    "\n",
    "        if x[col[feature]] <= float(value):\n",
    "            cls = dictionary[q][0]                 \n",
    "        else:\n",
    "            cls = dictionary[q][1]\n",
    "\n",
    "\n",
    "        if isinstance(cls, dict):\n",
    "            rt = cls\n",
    "            return self.predict_row(x, rt)                     # recursive calling \n",
    "        else:\n",
    "            return cls\n",
    "    ### Predict Function - predicts testing dataset\n",
    "      #                 - Returns y_predicted list    \n",
    "    def predict(self, xtest):\n",
    "        output = np.zeros(len(xtest), dtype = int)\n",
    "        for i in range(len(xtest)):\n",
    "            row = xtest[i , :]\n",
    "            output[i] = self.predict_row(row, self.dictionary)\n",
    "        return output                                          # returning list of predicted classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 0\n",
      "Count of 0 =  37\n",
      "Current Entropy  is =  1.5848478277058313\n",
      "Splitting on feature petal length (cm) with gain ratio 1.442695040888963\n",
      "Count of 1 =  37\n",
      "Current Entropy  is =  1.5848478277058313\n",
      "Splitting on feature petal length (cm) with gain ratio 1.442695040888963\n",
      "Count of 2 =  38\n",
      "Current Entropy  is =  1.5848478277058313\n",
      "Splitting on feature petal length (cm) with gain ratio 1.442695040888963\n",
      "Level 1\n",
      "Count of 0 =  37\n",
      "Current Entropy  is =  0.0\n",
      "Reached leaf Node\n",
      "Level 1\n",
      "Count of 0 =  37\n",
      "Current Entropy  is =  0.999871756640849\n",
      "Splitting on feature petal width (cm) with gain ratio 0.9587053216903549\n",
      "Count of 1 =  38\n",
      "Current Entropy  is =  0.999871756640849\n",
      "Splitting on feature petal width (cm) with gain ratio 0.9587053216903549\n",
      "Level 2\n",
      "Count of 0 =  36\n",
      "Current Entropy  is =  0.4689955935892812\n",
      "Splitting on feature sepal length (cm) with gain ratio 0.7487424363630022\n",
      "Count of 1 =  4\n",
      "Current Entropy  is =  0.4689955935892812\n",
      "Splitting on feature sepal length (cm) with gain ratio 0.7487424363630022\n",
      "Level 3\n",
      "Count of 0 =  1\n",
      "Current Entropy  is =  0.0\n",
      "Reached leaf Node\n",
      "Level 3\n",
      "Count of 0 =  36\n",
      "Current Entropy  is =  0.39124356362925566\n",
      "Splitting on feature sepal width (cm) with gain ratio 0.06050815745843646\n",
      "Count of 1 =  3\n",
      "Current Entropy  is =  0.39124356362925566\n",
      "Splitting on feature sepal width (cm) with gain ratio 0.06050815745843646\n",
      "Level 4\n",
      "Count of 0 =  10\n",
      "Current Entropy  is =  0.0\n",
      "Reached leaf Node\n",
      "Level 4\n",
      "Count of 0 =  26\n",
      "Current Entropy  is =  0.4798320236161285\n",
      "Reached leaf Node\n",
      "Count of 1 =  3\n",
      "Current Entropy  is =  0.4798320236161285\n",
      "Reached leaf Node\n",
      "Level 2\n",
      "Count of 0 =  1\n",
      "Current Entropy  is =  0.18717625687320816\n",
      "Reached leaf Node\n",
      "Count of 1 =  34\n",
      "Current Entropy  is =  0.18717625687320816\n",
      "Reached leaf Node\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train, Y_train)\n",
    "ypred_cls = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'petal length (cm) <= 2.45': [0,\n",
       "  {'petal width (cm) <= 1.75': [{'sepal length (cm) <= 4.95': [2,\n",
       "      {'sepal width (cm) <= 2.55': [1, 1]}]},\n",
       "    2]}]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9473684210526315"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Y_test, ypred_cls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
